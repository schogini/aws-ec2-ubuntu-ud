What is a Pod?
A pod (as in a pod of whales or pea pod) is a group of one or more containers 
(such as Docker containers), with shared storage/network, and a specification 
for how to run the containers. A pod’s contents are always co-located and 
co-scheduled, and run in a shared context. A pod models an application-specific 
“logical host” - it contains one or more application containers which are relatively 
tightly coupled — in a pre-container world, they would have executed on the same 
physical or virtual machine.

Containers within a pod share an IP address and port space, and can find each other via localhost.
These containers usually communicate with each other via Pod IP addresses.

Applications within a pod also have access to shared volumes, which are defined 
as part of a pod and are made available to be mounted into each application’s 
filesystem.

In terms of Docker constructs, a pod is modelled as a group of Docker containers 
with shared namespaces and shared volumes.

Like individual application containers, pods are considered to be relatively 
ephemeral (rather than durable) entities.

pods are created, assigned a unique ID (UID), and scheduled to nodes where they 
remain until termination (according to restart policy) or deletion. If a node dies, 
the pods scheduled to that node are scheduled for deletion, it can be replaced by 
an identical pod, with even the same name if desired, but with a new UID (by the 
replication controller).

When something is said to have the same lifetime as a pod, such as a volume, that 
means that it exists as long as that pod (with that UID) exists. If that pod is 
deleted for any reason, even if an identical replacement is created, the related 
thing (e.g. volume) is also destroyed and created anew.

Pods can be used to host vertically integrated application stacks (e.g. LAMP), 
but their primary motivation is to support co-located, co-managed helper programs.

Why not just run multiple programs in a single (Docker) container?

Transparency. Making the containers within the pod visible to the infrastructure 
enables the infrastructure to provide services to those containers, such as process 
management and resource monitoring. This facilitates a number of conveniences for users.
Decoupling software dependencies. The individual containers may be versioned, rebuilt 
and redeployed independently. Kubernetes may even support live updates of individual 
containers someday.
Ease of use. Users don’t need to run their own process managers, worry about signal 
and exit-code propagation, etc.
Efficiency. Because the infrastructure takes on more responsibility, containers 
can be lighter weight.

Why not support affinity-based co-scheduling of containers?
That approach would provide co-location, but would not provide most of the benefits of 
pods, such as resource sharing, IPC, guaranteed fate sharing, and simplified management.

Durability of pods (or lack thereof)
Pods aren’t intended to be treated as durable entities. They won’t survive scheduling 
failures, node failures, or other evictions, such as due to lack of resources, or in 
the case of node maintenance.

In general, users shouldn’t need to create pods directly. They should almost always 
use controllers eg: via Deployments). Controllers provide 
self-healing with a cluster scope, as well as replication and rollout management. 
Controllers like StatefulSet can also provide support to stateful pods.

Termination of Pods
Because pods represent running processes on nodes in the cluster, it is important to 
allow those processes to gracefully terminate when they are no longer needed (vs being 
violently killed with a KILL signal and having no chance to clean up). Users should be 
able to request deletion and know when processes terminate, but also be able to ensure 
that deletes eventually complete. When a user requests deletion of a pod the system 
records the intended grace period before the pod is allowed to be forcefully killed, 
and a TERM signal is sent to the main process in each container

